{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhaRahul2102/Emotion-detection-using-streamlit-and-keras/blob/main/Facial_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "menBs1sKB36k",
        "outputId": "66978d1e-5ee9-411f-d13a-5db56acf8872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fer2013.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d msambare/fer2013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJBgHl8KCGXL"
      },
      "outputs": [],
      "source": [
        "import zipfile,os\n",
        "\n",
        "data=zipfile.ZipFile('/content/fer2013.zip')\n",
        "data.extractall('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PGm8KEmCvbD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BB_9ycs1p2Z"
      },
      "outputs": [],
      "source": [
        "model=keras.models.Sequential([\n",
        "    keras.layers.Conv2D(16,(3,3),activation='relu',padding='same',input_shape=(30,30,1)),\n",
        "    keras.layers.Conv2D(32,(3,3),activation='relu',padding='same'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    keras.layers.Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    keras.layers.Conv2D(128,(3,3),activation='relu',padding='same'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128,activation='relu'),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(7,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(learning_rate=0.001),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLAhdKUaDN5c",
        "outputId": "ad36d128-9b1e-455c-e775-28a95c86d565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_d=ImageDataGenerator(rescale=1.0/255.0,\n",
        "                           rotation_range=60,\n",
        "                           width_shift_range=0.2,\n",
        "                           height_shift_range=0.2,\n",
        "                           shear_range=0.2,\n",
        "                           zoom_range=0.2,\n",
        "                           horizontal_flip=True,\n",
        "                           fill_mode='nearest')\n",
        "\n",
        "valid_d=ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "train_gen=train_d.flow_from_directory('/content/train',target_size=(30,30),batch_size=145,class_mode='categorical',color_mode = \"grayscale\")\n",
        "valid_gen=train_d.flow_from_directory('/content/test',target_size=(30,30),batch_size=55,class_mode='categorical',color_mode = \"grayscale\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ch5K3sBGXga"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "\n",
        "early_stop=EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True)\n",
        "model_check=ModelCheckpoint(filepath='/content/mood1.h5',monitor='val_loss',save_best_only=True)\n",
        "lr=ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faz_ecX5w2ol"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw9MfPweGdLF",
        "outputId": "f0572944-a85b-4a78-8d38-811c64c6751b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "198/198 [==============================] - 92s 464ms/step - loss: 1.2749 - accuracy: 0.5157 - val_loss: 1.2837 - val_accuracy: 0.5086 - lr: 1.0000e-10\n",
            "Epoch 2/150\n",
            "198/198 [==============================] - 87s 440ms/step - loss: 1.2761 - accuracy: 0.5118 - val_loss: 1.2890 - val_accuracy: 0.5085 - lr: 1.0000e-10\n",
            "Epoch 3/150\n",
            "198/198 [==============================] - 88s 444ms/step - loss: 1.2689 - accuracy: 0.5158 - val_loss: 1.2862 - val_accuracy: 0.5104 - lr: 1.0000e-10\n",
            "Epoch 4/150\n",
            "198/198 [==============================] - 88s 443ms/step - loss: 1.2748 - accuracy: 0.5162 - val_loss: 1.2929 - val_accuracy: 0.5061 - lr: 1.0000e-10\n",
            "Epoch 5/150\n",
            "198/198 [==============================] - 87s 439ms/step - loss: 1.2790 - accuracy: 0.5154 - val_loss: 1.2854 - val_accuracy: 0.5072 - lr: 1.0000e-11\n",
            "Epoch 6/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2742 - accuracy: 0.5161 - val_loss: 1.2896 - val_accuracy: 0.5099 - lr: 1.0000e-11\n",
            "Epoch 7/150\n",
            "198/198 [==============================] - 86s 434ms/step - loss: 1.2725 - accuracy: 0.5189 - val_loss: 1.2830 - val_accuracy: 0.5178 - lr: 1.0000e-11\n",
            "Epoch 8/150\n",
            "198/198 [==============================] - 86s 435ms/step - loss: 1.2743 - accuracy: 0.5128 - val_loss: 1.2810 - val_accuracy: 0.5043 - lr: 1.0000e-11\n",
            "Epoch 9/150\n",
            "198/198 [==============================] - 86s 433ms/step - loss: 1.2763 - accuracy: 0.5161 - val_loss: 1.2840 - val_accuracy: 0.5103 - lr: 1.0000e-11\n",
            "Epoch 10/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2753 - accuracy: 0.5160 - val_loss: 1.2792 - val_accuracy: 0.5070 - lr: 1.0000e-11\n",
            "Epoch 11/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2781 - accuracy: 0.5151 - val_loss: 1.2773 - val_accuracy: 0.5152 - lr: 1.0000e-11\n",
            "Epoch 12/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2739 - accuracy: 0.5170 - val_loss: 1.2943 - val_accuracy: 0.5017 - lr: 1.0000e-11\n",
            "Epoch 13/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2774 - accuracy: 0.5129 - val_loss: 1.2862 - val_accuracy: 0.5064 - lr: 1.0000e-11\n",
            "Epoch 14/150\n",
            "198/198 [==============================] - 86s 435ms/step - loss: 1.2743 - accuracy: 0.5144 - val_loss: 1.2789 - val_accuracy: 0.5142 - lr: 1.0000e-11\n",
            "Epoch 15/150\n",
            "198/198 [==============================] - 86s 435ms/step - loss: 1.2700 - accuracy: 0.5148 - val_loss: 1.2823 - val_accuracy: 0.5106 - lr: 1.0000e-12\n",
            "Epoch 16/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2684 - accuracy: 0.5184 - val_loss: 1.2879 - val_accuracy: 0.5045 - lr: 1.0000e-12\n",
            "Epoch 17/150\n",
            "198/198 [==============================] - 87s 439ms/step - loss: 1.2764 - accuracy: 0.5123 - val_loss: 1.2865 - val_accuracy: 0.5049 - lr: 1.0000e-12\n",
            "Epoch 18/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2760 - accuracy: 0.5147 - val_loss: 1.2783 - val_accuracy: 0.5109 - lr: 1.0000e-13\n",
            "Epoch 19/150\n",
            "198/198 [==============================] - 86s 436ms/step - loss: 1.2772 - accuracy: 0.5155 - val_loss: 1.2906 - val_accuracy: 0.5093 - lr: 1.0000e-13\n",
            "Epoch 20/150\n",
            "198/198 [==============================] - 87s 437ms/step - loss: 1.2779 - accuracy: 0.5140 - val_loss: 1.2866 - val_accuracy: 0.5021 - lr: 1.0000e-13\n",
            "Epoch 21/150\n",
            "198/198 [==============================] - 87s 439ms/step - loss: 1.2746 - accuracy: 0.5134 - val_loss: 1.2931 - val_accuracy: 0.5036 - lr: 1.0000e-14\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(train_gen,epochs=150,validation_data=valid_gen,shuffle=True,callbacks=[early_stop,model_check,lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "cdfc2tEQGqo2",
        "outputId": "9538f884-fe85-4e52-a8be-edce72c5eb5b"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e8a27b6cf865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.models' has no attribute 'optimizers'"
          ]
        }
      ],
      "source": [
        "keras.models.optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eldtNlWBnAHs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Facial Training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwk/uJUvoAzcPxIjWcDIWq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}